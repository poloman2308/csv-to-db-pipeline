# ğŸ“¦ CSV-to-Database ETL Pipeline

[![Python](https://img.shields.io/badge/python-3.12-blue?logo=python)](https://www.python.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Build Status](https://img.shields.io/badge/build-passing-brightgreen)](https://github.com/poloman2308/csv-to-db-pipeline/actions)
[![Test Coverage](https://img.shields.io/badge/coverage-100%25-blue)](https://github.com/poloman2308/csv-to-db-pipeline)

A lightweight data engineering pipeline that loads CSV files into a PostgreSQL database using **Python, Pandas, SQLAlchemy**, and exposes a triggerable **Flask API**.

---

## ğŸš€ Features

- CLI and API-based CSV ingestion
- Data transformation via Pandas
- Loads into PostgreSQL using SQLAlchemy
- Logging for all ETL events
- `.env`-based configuration
- Containerized PostgreSQL support via Docker

---

## ğŸ§± Project Structure

```plaintext
csv_to_db_pipeline/
â”œâ”€â”€ api/               # Flask API to trigger pipeline
â”‚   â””â”€â”€ app.py
â”œâ”€â”€ etl/               # Core ETL logic
â”‚   â””â”€â”€ load_data.py
â”œâ”€â”€ data/              # Sample or target CSV files
â”œâ”€â”€ test_request.py    # Script to test the API locally
â”œâ”€â”€ .env               # DB connection credentials (not committed)
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## End-to-End Flow Diagram

```mermaid
flowchart LR
  %% ---------- INGEST ----------
  subgraph Ingest
    C1[data/customers.csv] -->|CLI ğŸ§©| L1[load_data.py<br/>--csv --table]
    C2[data/orders.csv]   -->|API ğŸš€| API[/POST /load<br/>app.py/]
  end

  %% ---------- TRANSFORM & LOAD ----------
  subgraph ETL Engine
    L1 --> T1[Pandas<br/>clean/transform]
    API --> T2[Pandas<br/>validate/normalize]
    T1 --> DB[(PostgreSQL<br/>raw_customers)]
    T2 --> DB
  end

  %% ---------- MONITORING ----------
  subgraph Logging
    L1 --> LOG1[(console.log ğŸ“‹)]
    API --> LOG2[(Flask log ğŸ“‹)]
  end

  style C1 fill:#fffbe7,stroke:#444
  style C2 fill:#fffbe7,stroke:#444
  style L1 fill:#dde1ff,stroke:#444
  style API fill:#dde1ff,stroke:#444
  style T1 fill:#e7ffe7,stroke:#444
  style T2 fill:#e7ffe7,stroke:#444
  style DB fill:#ffe7e7,stroke:#444
  style LOG1 fill:#e7f5ff,stroke:#444
  style LOG2 fill:#e7f5ff,stroke:#444
```

---

## âš™ï¸ .env Format

```yaml
DB_USER=dbt_user
DB_PASSWORD=dbt_pass
DB_NAME=ecommerce
DB_HOST=localhost
DB_PORT=5433
```
> ğŸ” This file should be excluded from Git (.gitignore).

---

## ğŸ–¥ï¸ Run Locally

```
python etl/load_data.py --csv data/customers.csv --table raw_customers
python etl/load_data.py --csv data/orders.csv --table raw_orders
python etl/load_data.py --csv data/products.csv --table raw_products
```

---

## â–¶ï¸ Flask API:

```bash
python api/app.py
```

---

## ğŸ” Make a request:

```
curl -X POST http://localhost:5000/load \
     -H "Content-Type: application/json" \
     -d "{\"csv_path\": \"data/customers.csv\", \"table_name\": \"raw_customers\"}"
```

---

## ğŸ³ Optional: Start PostgreSQL via Docker

```bash
docker run --name etl-postgres -e POSTGRES_USER=dbt_user -e POSTGRES_PASSWORD=dbt_pass -e POSTGRES_DB=ecommerce -p 5433:5432 -d postgres:15
```

---

## âœ… Example Response

```json
{
  "message": "Loaded data/customers.csv into raw_customers"
}
```

---

## âœï¸ Author

**Derek Acevedo**  
ğŸ“ [GitHub](https://github.com/poloman2308)  
ğŸ“„ [LinkedIn](https://www.linkedin.com/in/derekacevedo86)


